import asyncio
import csv
import os
import re
from urllib.parse import urlparse

import requests
from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig

# ⬅️ URL PUBLICADA COMO CSV de tu Google Sheet
GOOGLE_SHEET_CSV_URL = (
    "https://docs.google.com/spreadsheets/d/e/2PACX-1vTy7ua0I8u3h4alZS9tbogYrtt6Cu5Pv7q2q_g3kuSuv0Yg4ca6uN5ebU8t6MewL5ELXzeQNXj13xtl/pub?gid=0&single=true&output=csv"
)

# Carpeta donde se guardarán los .md para NotebookLM
OUTPUT_DIR = "notebooklm_crawls"


def load_urls_and_names_from_sheet(csv_url: str):
    """
    Descarga el CSV público de Google Sheets y devuelve
    una lista de (url, base_filename).
    """
    print(f"Descargando URLs desde Google Sheets...\n{csv_url}\n")
    resp = requests.get(csv_url, timeout=30)
    resp.raise_for_status()

    pairs: list[tuple[str, str]] = []
    reader = csv.reader(resp.text.splitlines())

    first_row = True
    for row in reader:
        if not row:
            continue

        # Esperamos al menos una columna (URL)
        url_cell = row[0].strip()
        name_cell = row[1].strip() if len(row) > 1 else ""

        # Saltar la cabecera (primera fila) si no empieza por http
        if first_row and not url_cell.startswith(("http://", "https://")):
            first_row = False
            continue
        first_row = False

        if not url_cell.startswith(("http://", "https://")):
            continue

        # Si no hay nombre en la segunda columna, lo generamos desde la URL
        if not name_cell:
            name_cell = safe_basename_from_url(url_cell)

        pairs.append((url_cell, name_cell))

    print(f"Encontradas {len(pairs)} filas válidas (URL + nombre).\n")
    return pairs


def safe_basename_from_url(url: str) -> str:
    """
    Convierte una URL en un nombre base de archivo válido (sin extensión).
    """
    parsed = urlparse(url)
    base = (parsed.netloc + parsed.path).strip("/")
    if not base:
        base = "page"
    base = re.sub(r'[<>:"/\\|?*]+', "_", base)
    return base


def ensure_md_extension(name: str) -> str:
    """
    Asegura que el nombre de archivo termine en .md.
    """
    if not name.lower().endswith(".md"):
        name = name + ".md"
    return name


async def crawl_urls_to_markdown(pairs: list[tuple[str, str]]) -> None:
    """
    Usa Crawl4AI para convertir cada URL en Markdown limpio (fit_markdown/raw_markdown)
    y guardarlo en archivos .md dentro de OUTPUT_DIR.
    """
    os.makedirs(OUTPUT_DIR, exist_ok=True)

    browser_config = BrowserConfig(
        headless=True,
        java_script_enabled=True,
    )

    run_config = CrawlerRunConfig(
        wait_until="networkidle",
    )

    async with AsyncWebCrawler(config=browser_config) as crawler:
        total = len(pairs)
        for idx, (url, base_name) in enumerate(pairs, start=1):
            print(f"[{idx}/{total}] Crawling: {url}")
            result = await crawler.arun(url=url, config=run_config)

            if not getattr(result, "success", False):
                print(
                    f"  ✗ Error: {getattr(result, 'error_message', 'Error desconocido')}\n"
                )
                continue

            md = None
            # En versiones nuevas result.markdown puede ser str o un objeto
            md_obj = getattr(result, "markdown", None)
            if isinstance(md_obj, str):
                md = md_obj
            elif md_obj is not None:
                md = getattr(md_obj, "fit_markdown", None) or getattr(
                    md_obj, "raw_markdown", None
                )

            if not md:
                print("  ✗ No se obtuvo Markdown. Saltando.\n")
                continue

            filename = ensure_md_extension(base_name)
            out_path = os.path.join(OUTPUT_DIR, filename)

            with open(out_path, "w", encoding="utf-8") as f:
                f.write(md)

            print(f"  ✓ Guardado en: {out_path}\n")


def main() -> None:
    pairs = load_urls_and_names_from_sheet(GOOGLE_SHEET_CSV_URL)
    if not pairs:
        print(
            "No hay filas válidas en la hoja. Revisa el CSV (URL en col A, nombre en col B)."
        )
        return

    asyncio.run(crawl_urls_to_markdown(pairs))
    print(f"Terminado. Archivos Markdown listos en: {OUTPUT_DIR}")


if __name__ == "__main__":
    main()
